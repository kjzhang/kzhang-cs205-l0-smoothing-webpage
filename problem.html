<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="description" content="CS 205 Final Project">
    <meta name="keywords" content="your,keywords,goes,here">
    <meta name="author" content="Kevin Zhang / Han He / Design: Andreas Viklund - http://andreasviklund.com/">
    <link rel="stylesheet" type="text/css" href="css/basic-modular.css">
    <title>CS 205 Final Project - Kevin Zhang &amp; Han He</title>
</head>

<body>
<!-- Layout setting -->
<div id="container">

<!-- Header block -->
<div class="header">
    <h1>Parallel Image Smoothing via L0 Gradient Minimization</h1><br>
    <p>CS 205 Final Project</p>
</div>
<!-- / Header block -->

<!-- Main menu block -->
<div class="mainmenu">
    <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="problem.html" class="active">Problem</a></li>
        <li><a href="design.html">Design</a></li>
        <li><a href="application.html">Application</a></li>
        <li><a href="performance.html">Performance</a></li>
        <li><a href="insights.html">Insights</a></li>
        <li><a href="conclusions.html">Conclusions</a></li>
    </ul>
</div>
<!-- / Main menu block -->


<!-- Single content block -->
<div class="singlecontent">
    <h2>Framework</h2>
    <p>
        The paper presents a general global smoothing method based on a sparsity measure, which
        corresponds to the L0-norm of the gradient. A constraint is placed upon the total number
        of non-zero gradients between pixels. The algorithm consists of two main features,
        flattening insignificant details by removing small non-zero gradients, and enhancing
        larger-scale edges, since large gradients are penalized the same amount as small gradients.
    </p>
    <p>
        First, we outline the framework in 1 dimension. We denote the discrete input image as g and
        the smoothed result by f. We create function c(f) for the number of amplitude changes
        between neighboring pixels p and p + 1:
    </p>
    <p>
        <center><img src="img/p1.png"></center>
    </p>
    <p>
        Next, we express our objective function. The c(f) = k signifies that the count of non-zero
        gradients in f is constrained to k. A larger k allows for a finer approximation of the input
        image.
    </p>
    <p>
        <center><img src="img/p2.png"></center>
    </p>
    <p>
        The cost in our objective function is the quadratic intensity difference term, the total
        difference sum between pixels of f and g. The cost term ensures that during smoothing, pixels
        are generally not allowed to have major color changes, meaning diminishing salient edges is
        usually prevented.
    </p>
    <p>
        In practice, the range of k is dependent on the image, so we modify our cost function to include
        the level of structure flattening as well. The weight lambda controls the weight of c(f). Thus,
        a large lambda results in very few edges. In general, the total number of non-zero gradients is
        inversely proportional to lambda.
    </p>
    <p>
        <center><img src="img/p3.png"></center>
    </p>
    <p>
        The framework can easily be extended to 2 dimensions. I is the input image while S is the
        smoothed result. C(S) measures the total L0-norm of the gradients between neighboring pixels:
    </p>
    <p>
        <center><img src="img/p4.png"></center>
    </p>
    <p>
        Similarly, our objective function in 2D is:
    </p>
    <p>
        <center><img src="img/p5.png"></center>
    </p>
    <h2>Approximation</h2>
    <p>
        The closed form solution to the global optimum is NP-hard, but an approximate solution can
        be reduced to minimizing a quadradic function, which is much simpler to solve. We introduce
        2 new auxiliary variables h and v corresponding to the gradients in the x and y direction
        to expand the original terms and update them iteratively. Our new objective function becomes
        the equation on the right. The new term beta controls the similarity between h, v and the
        corresponding gradients. When beta becomes large, our new function appraches the original
        objective on the left.
    </p>
    <p>
        <center><img src="img/p6.png"></center>
    </p>
    <h2>Algorithm</h2>
    <p>
        We limit the overall number of gradient changes in the image by performing an alternating
        iterative process to approximate our solution, first flattening insignificant details and then
        amplifying prominent edges within each iteration. The goal is to solve the minimization problem
        outlined above.
    </p>
    <p>
        Step 1: estimate (h, v) subproblem. We first solve for the h and v terms in our objective function.
        The subproblem determines the changes between pixels. The objective function for (h, v) is:
    </p>
    <p>
        <center><img src="img/p7.png"></center>
    </p>
    <p>
        Once again the objective function can be further decomposed. We compute the energy at each
        pixel, mask the values with a threshold dependent on lambda and beta, and then sum them to find
        the global optimum for the (h, v) objective function.
    </p>
    <p>
        Step 2: estimate S + 1 subproblem. After solving for the optimal (h, v) we can eliminate the terms
        from our objective function not involving S and solve for S + 1.
    </p>
    <p>
        <center><img src="img/p10.png"></center>
    </p>
    <p>
        The function is quadratic and has a global minimum. We can diagonalize derivative operators after FFT
        for speedup, avoiding having to compute large matrix inverses. F is the FFT operator, while F* is the
        complex conjugate.
    </p>
    <p>
        <center><img src="img/p9.png"></center>
    </p>
    <p>
        In conclusion, we have an alternating iterative process. We first solve for (h, v), and then solve for
        S + 1 in each iteration, while incrementing beta. Once beta reaches a threshold, we have convergence
        for our solution approximating the original objective function. More detailed theory about the algorithm
        can be found in the paper.
    </p>
</div>
<!-- / Single content block -->


<!-- Footer block -->
<div class="footer">
    <p>Website Â© 2012 Kevin Zhang &amp; Han He | Template by <a href="http://andreasviklund.com/">Andreas Viklund</a></p>
</div>
<!-- / Footer block -->

</div>
<!-- / Layout setting -->

</body>

</html>
